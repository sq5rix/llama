Prompt to create code

> write new class to implement new RAG system.
> I need to read all files from directory data
> Then, I need to know max context length for used model
> Use llama model, 3.1 7b 4-bit 
> Write methon to get max context size - if you cannot, use constant MAX_CONTEXT_SIZE=8192
> Write method to get files and split them into chunks sized 10% less 
> than context size
> For each chunk remember in dict file name of chunk, position in file
> For each chunk, execute llama model with system prompt
> CONTEXT_PROMPT = f"
> create prefix for this chunk 
> {chunk}
> In prefix tell about {file name} of chunk
> {position} of chunk in file
> Summary of chunk content, readable by AI
> "
> for each chunk create prefix+chunk
> Embed chunks using nomic model
> Persist chunks in chroma db
> Create main method infer to infer user prompt with rag
> Use cosine vector similarity with user prompt
> Create tests to proof the code
> 

I'll help you create a new RAG system with the specified requirements. I'll create a class called `ContextualRAG` with all the necessary methods and a separate test file.

Filepath: contextual_rag.py
Replace lines: 1-2
```python
"""
Contextual RAG system implementation with Llama model and chunk-based processing.
"""

import os
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ContextualRAG:
    MAX_CONTEXT_SIZE = 8192  # Default max context size for Llama models
    CHUNK_SIZE_FACTOR = 0.9  # 10% less than max context size

    def __init__(self, data_dir: str, persist_dir: str = "./chroma_db"):
        """
        Initialize ContextualRAG system.
        
        Args:
            data_dir: Directory containing the source files
            persist_dir: Directory to persist Chroma DB
        """
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        self.model = ChatOllama(model="llama2:7b-4bit")
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")
        self.chunk_metadata: Dict[str, Dict] = {}
        self.vectorstore = None
        
    def get_context_size(self) -> int:
        """Get the maximum context size for the model."""
        # In a real implementation, you might query the model for this
        return self.MAX_CONTEXT_SIZE
        
    def load_and_split_files(self) -> List[Tuple[str, str, int]]:
        """
        Load files from data directory and split into chunks.
        
        Returns:
            List of tuples containing (chunk_text, filename, position)
        """
        chunks_with_metadata = []
        chunk_size = int(self.get_context_size() * self.CHUNK_SIZE_FACTOR)
        
        text_splitter = CharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=200,
            length_function=len
        )
        
        for file_path in self.data_dir.glob("*.*"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    chunks = text_splitter.split_text(content)
                    
                    for position, chunk in enumerate(chunks):
                        chunks_with_metadata.append((
                            chunk,
                            file_path.name,
                            position
                        ))
                        
                except Exception as e:
                    logger.error(f"Error processing file {file_path}: {str(e)}")
                    
        return chunks_with_metadata

    def create_chunk_prefix(self, chunk: str, filename: str, position: int) -> str:
        """
        Create a prefix for each chunk using the model.
        
        Args:
            chunk: The text chunk
            filename: Source filename
            position: Position in the file
            
        Returns:
            Generated prefix
        """
        prompt_template = """Create a prefix for this chunk:
        
Content: {chunk}

The prefix should include:
- File: {filename}
- Position: {position}
- Summary of content (AI-readable)

Generate a concise prefix:"""

        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | self.model | StrOutputParser()
        
        try:
            prefix = chain.invoke({
                "chunk": chunk,
                "filename": filename,
                "position": position
            })
            return prefix.strip()
        except Exception as e:
            logger.error(f"Error creating prefix: {str(e)}")
            return f"File: {filename}, Position: {position}"

    def process_and_embed(self):
        """Process all files, create prefixes, and embed in Chroma DB."""
        chunks_with_metadata = self.load_and_split_files()
        processed_chunks = []
        
        for chunk, filename, position in chunks_with_metadata:
            prefix = self.create_chunk_prefix(chunk, filename, position)
            processed_text = f"{prefix}\n\n{chunk}"
            processed_chunks.append(processed_text)
            
            # Store metadata
            chunk_id = f"{filename}_{position}"
            self.chunk_metadata[chunk_id] = {
                "filename": filename,
                "position": position,
                "prefix": prefix
            }

        # Create and persist vector store
        self.vectorstore = Chroma.from_texts(
            texts=processed_chunks,
            embedding=self.embeddings,
            persist_directory=str(self.persist_dir)
        )
        self.vectorstore.persist()

    def infer(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        Infer using RAG with the user prompt.
        
        Args:
            query: User query
            top_k: Number of most similar chunks to return
            
        Returns:
            List of relevant chunks with their metadata
        """
        if not self.vectorstore:
            raise ValueError("Vector store not initialized. Run process_and_embed first.")
            
        # Get similar chunks using cosine similarity
        results = self.vectorstore.similarity_search_with_scores(
            query,
            k=top_k
        )
        
        response = []
        for doc, score in results:
            # Extract filename and position from the content
            content_lines = doc.page_content.split('\n')
            metadata = {}
            
            # Find matching chunk metadata
            for chunk_id, meta in self.chunk_metadata.items():
                if meta["prefix"] in doc.page_content:
                    metadata = meta
                    break
                    
            response.append({
                "content": doc.page_content,
                "similarity_score": score,
                "metadata": metadata
            })
            
        return response
```

Now, let's create a test file:

Filepath: test_contextual_rag.py
Replace lines: 1-0
```python
import unittest
import tempfile
import os
from pathlib import Path
from contextual_rag import ContextualRAG

class TestContextualRAG(unittest.TestCase):
    def setUp(self):
        # Create temporary directories for data and persistence
        self.temp_data_dir = tempfile.mkdtemp()
        self.temp_persist_dir = tempfile.mkdtemp()
        
        # Create some test files
        self.create_test_files()
        
        # Initialize RAG system
        self.rag = ContextualRAG(
            data_dir=self.temp_data_dir,
            persist_dir=self.temp_persist_dir
        )
        
    def tearDown(self):
        # Cleanup temporary directories
        import shutil
        shutil.rmtree(self.temp_data_dir)
        shutil.rmtree(self.temp_persist_dir)
        
    def create_test_files(self):
        # Create test file 1
        with open(os.path.join(self.temp_data_dir, "test1.txt"), "w") as f:
            f.write("This is a test document about artificial intelligence. " * 50)
            
        # Create test file 2
        with open(os.path.join(self.temp_data_dir, "test2.txt"), "w") as f:
            f.write("This document contains information about machine learning. " * 50)
            
    def test_context_size(self):
        size = self.rag.get_context_size()
        self.assertEqual(size, ContextualRAG.MAX_CONTEXT_SIZE)
        
    def test_load_and_split_files(self):
        chunks = self.rag.load_and_split_files()
        self.assertTrue(len(chunks) > 0)
        self.assertEqual(len(chunks[0]), 3)  # (chunk, filename, position)
        
    def test_create_chunk_prefix(self):
        chunk = "This is a test chunk about AI."
        filename = "test.txt"
        position = 0
        
        prefix = self.rag.create_chunk_prefix(chunk, filename, position)
        self.assertIsInstance(prefix, str)
        self.assertTrue(len(prefix) > 0)
        
    def test_process_and_embed(self):
        self.rag.process_and_embed()
        self.assertIsNotNone(self.rag.vectorstore)
        self.assertTrue(len(self.rag.chunk_metadata) > 0)
        
    def test_infer(self):
        self.rag.process_and_embed()
        results = self.rag.infer("What is artificial intelligence?")
        
        self.assertTrue(len(results) > 0)
        self.assertIn("content", results[0])
        self.assertIn("similarity_score", results[0])
        self.assertIn("metadata", results[0])

if __name__ == "__main__":
    unittest.main()
```

This implementation includes:

1. A `ContextualRAG` class with all required functionality:
   - Loading files from a directory
   - Splitting into chunks
   - Creating prefixes using Llama model
   - Embedding using nomic-embed-text
   - Persisting in Chroma DB
   - Inference with cosine similarity

2. Key features:
   - Configurable context size
   - Chunk size as 90% of context size
   - Metadata tracking for chunks
   - Error handling and logging
   - Persistent storage with Chroma DB

3. Comprehensive test suite that verifies:
   - Context size calculation
   - File loading and splitting
   - Prefix creation
   - Embedding process
   - Inference functionality

To use the system:

```python
# Initialize
rag = ContextualRAG(data_dir="./data")

# Process and embed documents
rag.process_and_embed()

# Query the system
results = rag.infer("Your query here")
for result in results:
    print(f"Content: {result['content']}")
    print(f"Score: {result['similarity_score']}")
    print(f"Metadata: {result['metadata']}")
```

Make sure to:
1. Create a `data` directory with your source files
2. Install required dependencies:
```bash
pip install langchain-community chromadb ollama
```

The system will automatically:
- Read all files from the data directory
- Split them into appropriate chunks
- Create AI-generated prefixes
- Embed and store in Chroma DB
- Provide similarity-based search

**Generation complete!** Please review the code suggestions above.


